{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nFace Recognition Using Unlabeled Data\\nCarmen Mart´ınez and Olac Fuentes\\nInstituto Nacional de Astrof´ısica, Optica y Electr ´ onica ´\\nLuis Enrique Erro # 1\\nSanta Mar´ıa Tonanzintla, Puebla, 72840, Mexico ´\\ncarmen@ccc.inaoep.mx, fuentes@inaoep.mx\\n\\n\\nFace recognition systems can normally attain good accuracy when they are provided with a large set of training\\nexamples. However, when a large training set is not available, their performance is commonly poor. In this work\\nwe describe a method for face recognition that achieves\\ngood results when only a very small training set is available (it can work with a training set as small as one image\\nper person). The method is based on augmenting the original training set with previously unlabeled data (that is, face\\nimages for which the identity of the person is not known).\\nInitially, we apply the well-known eigenfaces technique to\\nreduce the dimensionality of the image space, then we perform an iterative process, classifying all the unlabeled data\\nwith an ensemble of classifiers built from the current training set, and appending to the training set the previously\\nunlabeled examples that are believed to be correctly classified with a high confidence level, according to the ensemble. We experimented with ensembles based on the\\nk-nearest-neighbors, feedforward artificial neural networks\\nand locally weighted linear regression learning algorithms.\\nOur experimental results show that using unlabeled data\\nimproves the accuracy in all cases. The best accuracy,\\n92.07%, was obtained with locally weighted linear regression using 30 eigenfaces and appending 3 examples of every class in each iteration. In contrast, using only labeled\\ndata, an accuracy of only 34.81% was obtained .\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Face Recognition Using Unlabeled Data\n",
    "Carmen Mart´ınez and Olac Fuentes\n",
    "Instituto Nacional de Astrof´ısica, Optica y Electr ´ onica ´\n",
    "Luis Enrique Erro # 1\n",
    "Santa Mar´ıa Tonanzintla, Puebla, 72840, Mexico ´\n",
    "carmen@ccc.inaoep.mx, fuentes@inaoep.mx\n",
    "\n",
    "\n",
    "Face recognition systems can normally attain good accuracy when they are provided with a large set of training\n",
    "examples. However, when a large training set is not available, their performance is commonly poor. In this work\n",
    "we describe a method for face recognition that achieves\n",
    "good results when only a very small training set is available (it can work with a training set as small as one image\n",
    "per person). The method is based on augmenting the original training set with previously unlabeled data (that is, face\n",
    "images for which the identity of the person is not known).\n",
    "Initially, we apply the well-known eigenfaces technique to\n",
    "reduce the dimensionality of the image space, then we perform an iterative process, classifying all the unlabeled data\n",
    "with an ensemble of classifiers built from the current training set, and appending to the training set the previously\n",
    "unlabeled examples that are believed to be correctly classified with a high confidence level, according to the ensemble. We experimented with ensembles based on the\n",
    "k-nearest-neighbors, feedforward artificial neural networks\n",
    "and locally weighted linear regression learning algorithms.\n",
    "Our experimental results show that using unlabeled data\n",
    "improves the accuracy in all cases. The best accuracy,\n",
    "92.07%, was obtained with locally weighted linear regression using 30 eigenfaces and appending 3 examples of every class in each iteration. In contrast, using only labeled\n",
    "data, an accuracy of only 34.81% was obtained .\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/josephwoodall/ds_personal_projects/brooks_family_photo_project/brooks_family_photo_project_original_dataset/jpeg/model_images/*.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def linear_regression_face_clustering():\n",
    "\n",
    "        # importing the libraries\n",
    "        import matplotlib.cm as cm\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        from scipy import misc\n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "        # importing the data\n",
    "        with open(path) as f:\n",
    "            for line in f: \n",
    "                im = misc.imread(line.strip().split()[0])\n",
    "                testing_data.append(im.reshape(2500,))\n",
    "                testing_labels.append(line.strip().split()[1])\n",
    "\n",
    "        # mkdir train.txt, test.txt, '.../model_images/path'\n",
    "\n",
    "        testing_data, testing_labels = np.array(testing_data, dtype = float), np.array(testing_labels, dtype = int)\n",
    "\n",
    "        mean_subtracted_data = testing_data\n",
    "        for x in range(0, testing_data.shape[0]):\n",
    "            mean_subtracted_data[x, :] = mean_subtracted_data[x, :] - average_face\n",
    "\n",
    "        r = num\n",
    "        f_feature_matrix_test = mean_subtracted_data.dot(np.transpose(eigen_face[:r, :])\n",
    "        return(f_feature_matrix_test, testing_labels)\n",
    "\n",
    "        train_labels, train_data = [], []\n",
    "\n",
    "        with open('faces/train.txt') as f:\n",
    "            for line in f:\n",
    "                  im = misc.imread(line.strip().split()[0])\n",
    "                  train_data.append(im.reshape(2500, ))\n",
    "                  train_labels.append(line.strip().strip()[1])\n",
    "\n",
    "        average_face = np.zeros((2500,), dtype = float)\n",
    "    for x in range(0, train_data.shape[0]):\n",
    "        average_face = average_face + train_data[x, :]\n",
    "\n",
    "    average_face = average_face/train_data.shape[0]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title('Average Face')\n",
    "    plt.imshow(average_face.reshape(50, 50), cmap = cm.Greys_r)\n",
    "    # plt.show()\n",
    "\n",
    "    # part d\n",
    "    mean_subtracted_data = train_data\n",
    "    for x in range(0, train_data.shape[0]):\n",
    "        mean_subtracted_data[x, :] = mean_subtracted_data[x, :] - average_face\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title('Mean Subtracted Face')\n",
    "    plt.imshow(mean_subtracted_data[10,:].reshape(50, 50), cmap = cm.Greys_r)\n",
    "\n",
    "    # part e\n",
    "    U, Sigma, Vt = np.linalg.svd(mean_subtracted_data)\n",
    "    plt.figure()\n",
    "    for i in range(0, 10):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(Vt[i, :].reshape(50, 50), cmap = cm.Greys_r)\n",
    "    plt.title('10 Eigen Faces')\n",
    "    plt.show()\n",
    "\n",
    "    # part f\n",
    "    # low_ranx_approximation = np.zeros(mean_subtracted_data.shape)\n",
    "    # error = np.zeros(200)\n",
    "    # for r in range(1, 200):\n",
    "    #     # print('Now evaluating r = '+str(r))\n",
    "    #     Zeta = np.zeros((r,r))\n",
    "    #     for i in range(0, r):\n",
    "    #         Zeta[i, i] = Sigma[i]\n",
    "    #     low_ranx_approximation = (U[:,:r].dot(Zeta)).dot(Vt[:r,:])\n",
    "    #     for i in range(0, mean_subtracted_data.shape[0]):\n",
    "    #         for j in range(0, mean_subtracted_data.shape[1]):\n",
    "    #             aij = (low_ranx_approximation[i][j] - mean_subtracted_data[i][j])\n",
    "    #             error[r-1] = error[r-1] + aij*aij\n",
    "    #     error[r-1] = np.sqrt(error[r-1])\n",
    "    # plt1.figure()\n",
    "    # plt1.plot(np.linspace(1,200,200), error)\n",
    "    # plt1.title('Error vs r')\n",
    "    # plt1.xlabel('r ->')\n",
    "    # plt1.ylabel('||X-X_r|| ->')\n",
    "    # plt1.show()\n",
    "\n",
    "    # Part G\n",
    "    r=10\n",
    "    print('r = '+str(r))\n",
    "    F_feature_matrix_train = mean_subtracted_data.dot(np.transpose(Vt[:r,:]))\n",
    "    F_feature_matrix_test, test_labels = get_feature_matrix('faces/test.txt', r, average_face, Vt)\n",
    "\n",
    "    # Part H\n",
    "    logistic_regression_model = LogisticRegression()\n",
    "    logistic_regression_model.fit(F_feature_matrix_train, train_labels)\n",
    "\n",
    "    test_acc = logistic_regression_model.score(F_feature_matrix_test, test_labels)\n",
    "    print('Testing Accuracy : '+str(test_acc*100)+' %')\n",
    "\n",
    "    # Part I\n",
    "    Accuracy = np.zeros(200)\n",
    "    for r in range(1,200):\n",
    "        F_feature_matrix_test, test_labels = get_feature_matrix('faces/test.txt', r, average_face, Vt)\n",
    "        F_feature_matrix_train = mean_subtracted_data.dot(np.transpose(Vt[:r, :]))\n",
    "        logistic_regression_model.fit(F_feature_matrix_train, train_labels)\n",
    "        Accuracy[r-1] = logistic_regression_model.score(F_feature_matrix_test, test_labels)*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
